{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extract Data from Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.12.0)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (44.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (25.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (2.3.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.11.0)\n",
      "Requirement already satisfied: packaging in /Users/polinakorobeinikova/Library/Python/3.12/lib/python/site-packages (from scrapy) (24.1)\n",
      "Requirement already satisfied: tldextract in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (5.3.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyOpenSSL>=22.0.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: automat>=24.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (3.17.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /Users/polinakorobeinikova/IU/Data Wrangling and Visualisation/Assignment 1/1 Part/highest_grossing_films\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject highest_grossing_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Spider \n",
    "Create *films_list.py* for parsing data in the directory *highest_grossing_films/highest_grossing_films/spiders* with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "\n",
    "# class for scraped data\n",
    "class Film(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    release_year = scrapy.Field()\n",
    "    directors = scrapy.Field()\n",
    "    box_office = scrapy.Field()\n",
    "    country = scrapy.Field()\n",
    "    attributes = scrapy.Field()\n",
    "    wiki_link = scrapy.Field()\n",
    "    running_time = scrapy.Field()\n",
    "    actors = scrapy.Field()\n",
    "\n",
    "\n",
    "\n",
    "# spider class\n",
    "class FilmSpider(scrapy.Spider):\n",
    "    name = \"films_list\"\n",
    "\n",
    "    # list of allowed domains for the spider\n",
    "    allowed_domains = [\"en.wikipedia.org\"]\n",
    "\n",
    "    # URLs to start crawling from\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"]\n",
    "\n",
    "    # callback function to handle the response from the start URLs\n",
    "    def parse(self, response):\n",
    "        # extract the needed table with 50 film entries\n",
    "        table = response.xpath('//table[contains(@class, \"wikitable\")][normalize-space(.//caption/text())=\"Highest-grossing films\"]')\n",
    "        # error if table was not found\n",
    "        if not table:\n",
    "            self.logger.error(\"Could not find the exact 'Highest-grossing films' table!\")\n",
    "            return\n",
    "\n",
    "        # extract rows from the table\n",
    "        rows = table.xpath('./tbody/tr')\n",
    "        # list to store the scraped items\n",
    "        items = []\n",
    "\n",
    "        # iterate over each table element\n",
    "        for entry in rows:\n",
    "            # get title, film link on Wikipedia, release year and box office from the table row\n",
    "            title = entry.xpath('.//th//span//a/text() | .//th//i/a/text() | .//th//a/text()').get()\n",
    "            wiki_link = entry.xpath('.//th//span//a/@href | .//th//i/a/@href | .//th//a/@href').get()\n",
    "            release_year = entry.xpath('normalize-space(.//td[4]//text())').get() \n",
    "            box_office_texts = entry.xpath('.//td[3]//descendant-or-self::text()').getall()\n",
    "            box_office = self.clean_box_office(\" \".join(box_office_texts).strip())\n",
    "\n",
    "            if title and wiki_link:\n",
    "                full_wiki_link = response.urljoin(wiki_link)  # convert to full URL\n",
    "\n",
    "                # yield request to scrape film's Wikipedia page to find more information\n",
    "                yield response.follow(full_wiki_link, callback=self.parse_film_page, meta={\n",
    "                    'title': title.strip(),\n",
    "                    'release_year': release_year.strip() if release_year else None,\n",
    "                    'box_office': box_office.strip() if box_office else None,\n",
    "                    'wiki_link': full_wiki_link\n",
    "                })\n",
    "            else: # title or link was not found\n",
    "                print(\"ERROR\", entry)\n",
    "\n",
    "\n",
    "    # scrape information from the film's page\n",
    "    def parse_film_page(self, response):\n",
    "        # scrape director(s)\n",
    "        directors = response.xpath(\n",
    "            '//th[contains(text(), \"Directed by\")]/following-sibling::td//a/descendant-or-self::text()'\n",
    "        ).getall()\n",
    "        # clean entries from brackets\n",
    "        directors_cleaned = []\n",
    "        for d in directors:\n",
    "            clean_text = d.strip()\n",
    "            if clean_text and clean_text not in [\"[\", \"]\"] and not clean_text.isdigit():  \n",
    "                directors_cleaned.append(clean_text)\n",
    "\n",
    "        directors = ', '.join(directors_cleaned) if directors else None\n",
    "\n",
    "        # extract country\n",
    "        country_xpath = response.xpath('//th[contains(text(), \"Country\") or contains(text(), \"Countries\")]/following-sibling::td')\n",
    "        country_texts = country_xpath.xpath('.//li/text() | .//text()').getall()\n",
    "\n",
    "        # clean the extracted text\n",
    "        country_cleaned = []\n",
    "        for c in country_texts:\n",
    "            clean_text = c.strip()\n",
    "            if clean_text and clean_text not in [\"[\", \"]\"] and not clean_text.isdigit():  \n",
    "                country_cleaned.append(clean_text)\n",
    "\n",
    "        country = ', '.join(country_cleaned) if country_cleaned else None\n",
    "\n",
    "        # extract running time\n",
    "        runtime = response.xpath('//th[div[contains(text(), \"Running time\")]]/following-sibling::td//text()').get()\n",
    "\n",
    "        # extract actors\n",
    "        actors = response.xpath('//th[contains(text(), \"Starring\")]/following-sibling::td//a/text()').getall()\n",
    "        actors = ', '.join(actor.strip() for actor in actors if actor.strip()) if actors else None\n",
    "\n",
    "\n",
    "        yield Film(\n",
    "            title=response.meta['title'],\n",
    "            release_year=response.meta['release_year'],\n",
    "            box_office=response.meta['box_office'],\n",
    "            wiki_link=response.meta['wiki_link'],\n",
    "            directors=directors,\n",
    "            country=country,\n",
    "            running_time = runtime,\n",
    "            actors=actors\n",
    "        )\n",
    "\n",
    "    # clean box office from '$'\n",
    "    def clean_box_office(self, value):\n",
    "        return re.sub(r\"[^\\d.]\", \"\", value) if value else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the Spider \n",
    "Run the following command in folder *highest_grossing_films* in your terminal to execute the Scrapy spider:\n",
    "\n",
    "    scrapy crawl films_list -o films.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean data\n",
    "1. Convert *release year* and *box office* to integers\n",
    "2. Extract only number of minutes in *running time*\n",
    "3. Clean *directors*\n",
    "4. Replace null values in *actors* with \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"highest_grossing_films/films.json\" \n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "# convert 'release_year' to integer\n",
    "df[\"release_year\"] = pd.to_numeric(df[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# convert 'box_office' to integer and fix incorrect values\n",
    "df[\"box_office\"] = pd.to_numeric(df[\"box_office\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df.loc[df[\"title\"] == \"The Fate of the Furious\", \"box_office\"] = df.loc[df[\"title\"] == \"The Fate of the Furious\", \"box_office\"].astype(str).str.lstrip(\"8\").astype(\"Int64\")\n",
    "\n",
    "# extract only numeric values from 'running_time'\n",
    "df[\"running_time\"] = df[\"running_time\"].apply(lambda x: int(x.split()[0]) if isinstance(x, str) and x.split()[0].isdigit() else None)\n",
    "\n",
    "# clean 'directors' by removing unwanted HTML artifacts\n",
    "df[\"directors\"] = df[\"directors\"].str.replace(r\".mw-parser-output.*?, \", \"\", regex=True)\n",
    "\n",
    "# replace null values in 'actors' with \"Unknown\"\n",
    "df[\"actors\"] = df[\"actors\"].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# save the cleaned dataset as a new JSON file\n",
    "cleaned_file_path = \"cleaned_films.json\"\n",
    "df.to_json(cleaned_file_path, orient=\"records\", indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when data cleaning is done, we can proceed to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQLite is **lightweight** and does not require a separate server.\n",
    "- SQLite allows us to store and query structured data efficiently.\n",
    "- It integrates well with Python using the `sqlite3` library.\n",
    "\n",
    "---\n",
    "\n",
    "Create a new **SQLite database (`films.db`)** and define a relational table **`films`** with the following schema:\n",
    "\n",
    "| Column        | Data Type   | Description |\n",
    "|--------------|------------|-------------|\n",
    "| `id`         | `INTEGER` (Primary Key) | Unique ID for each film |\n",
    "| `title`      | `TEXT NOT NULL` | The movie title |\n",
    "| `release_year` | `INTEGER` | Year of release |\n",
    "| `director`   | `TEXT` | Name(s) of the director(s) |\n",
    "| `box_office` | `REAL` | Box office revenue (cleaned) |\n",
    "| `country`    | `TEXT` | Country(s) of origin |\n",
    "| `running_time` | `INTEGER` | Film running time in minutes |\n",
    "| `actors` | `TEXT` | Actors starring in the film |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films.db'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# define the SQLite database file\n",
    "db_file = \"films.db\"\n",
    "\n",
    "# connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# create the 'films' table following the specified schema\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS films (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT NOT NULL,\n",
    "        release_year INTEGER,\n",
    "        director TEXT,\n",
    "        box_office REAL,\n",
    "        country TEXT,\n",
    "        running_time INTEGER,\n",
    "        actors TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# insert cleaned data into the 'films' table\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO films (title, release_year, director, box_office, country, running_time, actors) \n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (row[\"title\"], row[\"release_year\"], row[\"directors\"], row[\"box_office\"], row[\"country\"], row['running_time'], row['actors']))\n",
    "\n",
    "# commit and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# confirm database creation\n",
    "db_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check whether the data was inserted in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                        title  release_year  \\\n",
      "0   1                                       Avatar          2009   \n",
      "1   2                                  Toy Story 3          2010   \n",
      "2   3                                  Toy Story 4          2019   \n",
      "3   4   Pirates of the Caribbean: Dead Man's Chest          2006   \n",
      "4   5                                      Aladdin          2019   \n",
      "5   6  Pirates of the Caribbean: On Stranger Tides          2011   \n",
      "6   7                 Rogue One: A Star Wars Story          2016   \n",
      "7   8                                      Moana 2          2024   \n",
      "8   9    Star Wars: Episode I – The Phantom Menace          1999   \n",
      "9  10                                Jurassic Park          1993   \n",
      "\n",
      "            director    box_office                        country  \\\n",
      "0      James Cameron  2.923706e+09  United Kingdom, United States   \n",
      "1        Lee Unkrich  1.066971e+09                  United States   \n",
      "2        Josh Cooley  1.073395e+09                  United States   \n",
      "3     Gore Verbinski  1.066180e+09                  United States   \n",
      "4        Guy Ritchie  1.050694e+09                  United States   \n",
      "5       Rob Marshall  1.045714e+09                  United States   \n",
      "6     Gareth Edwards  1.057420e+09                  United States   \n",
      "7  David Derrick Jr.  1.053422e+09                  United States   \n",
      "8       George Lucas  1.046515e+09                  United States   \n",
      "9   Steven Spielberg  1.037535e+09                  United States   \n",
      "\n",
      "   running_time                                             actors  \n",
      "0           162  Sam Worthington, Zoe Saldana, Stephen Lang, Mi...  \n",
      "1           103  Tom Hanks, Tim Allen, Joan Cusack, Don Rickles...  \n",
      "2           100  Tom Hanks, Tim Allen, Annie Potts, Tony Hale, ...  \n",
      "3           151  Johnny Depp, Orlando Bloom, Keira Knightley, S...  \n",
      "4           128  Will Smith, Mena Massoud, Naomi Scott, Marwan ...  \n",
      "5           137  Johnny Depp, Penélope Cruz, Ian McShane, Kevin...  \n",
      "6           134  Felicity Jones, Diego Luna, Ben Mendelsohn, Do...  \n",
      "7           100                    Auliʻi Cravalho, Dwayne Johnson  \n",
      "8           134  Liam Neeson, Ewan McGregor, Natalie Portman, J...  \n",
      "9           127  Sam Neill, Laura Dern, Jeff Goldblum, Richard ...  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# connect to the database\n",
    "conn = sqlite3.connect(\"films.db\")\n",
    "\n",
    "# query to fetch all films\n",
    "query = \"SELECT * FROM films LIMIT 10;\"  # Fetch first 10 records\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# close the connection\n",
    "conn.close()\n",
    "\n",
    "# display the results\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
